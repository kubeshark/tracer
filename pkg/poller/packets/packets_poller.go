package packets

import (
	"encoding/binary"
	"fmt"
	"os"
	"sync"
	"time"
	"unsafe"

	"github.com/cilium/ebpf"
	"github.com/cilium/ebpf/perf"
	"github.com/go-errors/errors"

	"github.com/kubeshark/gopacket"
	"github.com/kubeshark/gopacket/layers"
	"github.com/kubeshark/tracer/internal/tai"
	"github.com/kubeshark/tracer/pkg/bpf"
	"github.com/kubeshark/tracer/pkg/rawpacket"
	"github.com/kubeshark/tracerproto/pkg/unixpacket"
	"github.com/rs/zerolog/log"
)

// Buffer pool for pktBuffer objects to avoid large allocations
var pktBufferPool = sync.Pool{
	New: func() interface{} {
		return &pktBuffer{}
	},
}

// preWarmPool pre-warms the pktBuffer pool with some initial objects
func preWarmPool() {
	// Pre-allocate a few pktBuffer objects to reduce initial allocation pressure
	for i := 0; i < 10; i++ {
		pktBufferPool.Put(&pktBuffer{})
	}
}

type tracerPacketsData struct {
	Timestamp uint64
	CgroupID  uint64
	ID        uint64
	Len       uint32
	TotLen    uint32
	Counter   uint32
	Num       uint16
	Last      uint16
	IPHdrType uint16
	Direction uint8
	Data      [4096]uint8
}

type tracerPktChunk struct {
	cpu int
	buf []byte
}

type pktBuffer struct {
	id  uint64
	num uint16
	len uint32
	buf [64 * 1024]byte
}

// reset resets the pktBuffer for reuse
func (p *pktBuffer) reset() {
	p.id = 0
	p.num = 0
	// Only clear the portion of the buffer that was actually used
	// This is more efficient than clearing the entire 64KB buffer
	if p.len > 0 {
		clear(p.buf[:p.len])
		p.len = 0
	}
}

type PacketsPoller struct {
	ethernetDecoder gopacket.Decoder
	ethhdrContent   []byte
	// DecodingLayerParser for better performance
	parser        *gopacket.DecodingLayerParser
	decodedLayers []gopacket.LayerType
	// Reusable layer objects
	ethLayer     layers.Ethernet
	ipv4Layer    layers.IPv4
	ipv6Layer    layers.IPv6
	icmpv4Layer  layers.ICMPv4
	icmpv6Layer  layers.ICMPv6
	tcpLayer     layers.TCP
	udpLayer     layers.UDP
	sctpLayer    layers.SCTP
	dnsLayer     layers.DNS
	radiusLayer  layers.RADIUS
	payloadLayer gopacket.Payload
	// Original fields
	mtx             sync.Mutex
	chunksReader    *perf.Reader
	gopacketWriter  bpf.GopacketWriter
	rawPacketWriter rawpacket.RawPacketWriter
	pktsMap         map[uint64]*pktBuffer // packet id to packet
	receivedPackets uint64
	lostChunks      uint64
	lastLostChunks  uint64
	lastLostCheck   time.Time
	tai             tai.TaiInfo
	stats           PacketsPollerStats
	// Reusable record to avoid allocations
	reusableRecord perf.Record
	pktBuf         []byte
}

type PacketsPollerStats struct {
	ChunksGot     uint64
	ChunksHandled uint64
	ChunksLost    uint64
	PacketsGot    uint64
}

func NewPacketsPoller(
	perfBuffer *ebpf.Map,
	gopacketWriter bpf.GopacketWriter,
	rawPacketWriter rawpacket.RawPacketWriter,
	perfBufferSize int,
) (*PacketsPoller, error) {
	var err error

	ethernetDecoder := gopacket.DecodersByLayerName["Ethernet"]
	if ethernetDecoder == nil {
		return nil, errors.New("Failed to get Ethernet decoder")
	}

	ethhdrContent := make([]byte, 14)

	poller := &PacketsPoller{
		ethernetDecoder: ethernetDecoder,
		ethhdrContent:   ethhdrContent,
		gopacketWriter:  gopacketWriter,
		rawPacketWriter: rawPacketWriter,
		pktsMap:         make(map[uint64]*pktBuffer),
		tai:             tai.NewTaiInfo(),
		decodedLayers:   make([]gopacket.LayerType, 0, 10),
		pktBuf:          make([]byte, 0, 14+64*1024),
	}

	// Initialize DecodingLayerParser for better performance
	poller.parser = gopacket.NewDecodingLayerParser(
		layers.LayerTypeEthernet,
		&poller.ethLayer,
		&poller.ipv4Layer,
		&poller.ipv6Layer,
		&poller.icmpv4Layer,
		&poller.icmpv6Layer,
		&poller.tcpLayer,
		&poller.udpLayer,
		&poller.sctpLayer,
		&poller.dnsLayer,
		&poller.radiusLayer,
		&poller.payloadLayer,
	)
	poller.parser.IgnoreUnsupported = true

	poller.chunksReader, err = perf.NewReader(perfBuffer, perfBufferSize)
	if err != nil {
		return nil, errors.Wrap(err, 0)
	}

	// Pre-warm the pool to reduce initial allocation pressure
	preWarmPool()

	return poller, nil
}

func (p *PacketsPoller) Stop() error {
	return p.chunksReader.Close()
}

func (p *PacketsPoller) Start() {
	go p.poll()
}

func (p *PacketsPoller) GetLostChunks() uint64 {
	return p.lostChunks
}

func (p *PacketsPoller) GetReceivedPackets() uint64 {
	return p.receivedPackets
}

func (p *PacketsPoller) GetExtendedStats() interface{} {
	return p.stats
}

func (p *PacketsPoller) poll() {
	// tracerPktsChunk is generated by bpf2go.

	go p.pollChunksPerfBuffer()
	go p.checkBuffers()
}

func (p *PacketsPoller) handlePktChunk(chunk tracerPktChunk) (bool, error) {
	p.mtx.Lock()
	defer p.mtx.Unlock()

	data := chunk.buf
	if len(data) == 4 {
		// zero packet to reset - return all pktBuffers to pool
		log.Info().Msg("Resetting plain packets buffer")
		for _, pkts := range p.pktsMap {
			pktBufferPool.Put(pkts)
		}
		p.pktsMap = make(map[uint64]*pktBuffer)
		return false, nil
	}
	const expectedChunkSize = 4148
	if len(data) != expectedChunkSize {
		return false, fmt.Errorf("bad pkt chunk: size %v expected: %v", len(data), expectedChunkSize)
	}

	ptr := (*tracerPacketsData)(unsafe.Pointer(&data[0]))

	pkts, ok := p.pktsMap[ptr.ID]
	if !ok {
		// Get pktBuffer from pool and initialize it
		pkts = pktBufferPool.Get().(*pktBuffer)
		pkts.reset()
		pkts.id = ptr.ID
		p.pktsMap[ptr.ID] = pkts
	}
	if ptr.Num != pkts.num {
		// chunk was lost
		log.Debug().Msgf("lost packet message id: (%v %v) num: (%v %v) len: %v last: %v dir: %v tot_len: %v cpu: %v", pkts.id, ptr.ID, pkts.num, ptr.Num, ptr.Len, ptr.Last, ptr.Direction, ptr.TotLen, chunk.cpu)
		return false, nil
	}

	copy(pkts.buf[pkts.len:], ptr.Data[:ptr.Len])
	pkts.len += uint32(ptr.Len)

	if ptr.Last != 0 {
		p.receivedPackets++

		binary.BigEndian.PutUint16(p.ethhdrContent[12:14], ptr.IPHdrType)

		if p.rawPacketWriter != nil {
			p.rawPacketWriter(ptr.Timestamp, pkts.buf[:pkts.len])
		}
		if p.gopacketWriter != nil {
			totalLen := 14 + int(pkts.len)
			if cap(p.pktBuf) < totalLen {
				// If pooled buffer is too small, allocate a new one
				p.pktBuf = make([]byte, totalLen)
			} else {
				p.pktBuf = p.pktBuf[:totalLen]
			}
			copy(p.pktBuf[:14], p.ethhdrContent)
			copy(p.pktBuf[14:], pkts.buf[:pkts.len])

			// Calculate timestamp once
			var timestamp time.Time
			if ptr.Timestamp != 0 {
				timestamp = time.Unix(0, int64(ptr.Timestamp)-int64(p.tai.GetTAIOffset()))
			} else {
				timestamp = time.Now()
			}

			// Use DecodingLayerParser for better performance - this avoids the overhead
			// of creating new layer objects and provides direct access to parsed data
			p.decodedLayers = p.decodedLayers[:0] // Reset slice but keep capacity

			parseErr := p.parser.DecodeLayers(p.pktBuf, &p.decodedLayers)

			if parseErr != nil {
				log.Fatal().Err(parseErr).Msg("DecodingLayerParser failed")
				return false, parseErr
			}
			// Create packet from decoded layers for optimal performance
			pkt := p.createPacketFromDecodedLayers(p.pktBuf, timestamp, ptr.CgroupID, unixpacket.PacketDirection(ptr.Direction))
			p.stats.PacketsGot++
			p.gopacketWriter(pkt)
		}

		// Return pktBuffer to pool for reuse
		pktBufferPool.Put(pkts)
		delete(p.pktsMap, ptr.ID)
	} else {
		pkts.num++
	}

	return true, nil
}

func (p *PacketsPoller) pollChunksPerfBuffer() {
	log.Info().Msg("Start polling for packet events")

	// remove all existing records
	p.chunksReader.SetDeadline(time.Unix(1, 0))
	var emptyRecord perf.Record
	for {
		err := p.chunksReader.ReadInto(&emptyRecord)
		if errors.Is(err, os.ErrDeadlineExceeded) {
			break
		} else if err != nil {
			log.Fatal().Err(err).Msg("Error reading chunks from pkts perf, aborting!")
			return
		}
	}
	p.chunksReader.SetDeadline(time.Time{})

	for {
		if time.Since(p.lastLostCheck) > time.Minute && p.lastLostChunks != p.lostChunks {
			log.Warn().Msg(fmt.Sprintf("Buffer is full, dropped %d chunks", p.lostChunks-p.lastLostChunks))
			p.lastLostChunks = p.lostChunks
			p.lastLostCheck = time.Now()
		}

		err := p.chunksReader.ReadInto(&p.reusableRecord)
		if err != nil {
			if errors.Is(err, perf.ErrClosed) {
				log.Info().Err(err).Msg("perf buffer is closed")
				return
			}

			log.Fatal().Err(err).Msg("Error reading chunks from pkts perf, aborting!")
			return
		}
		if p.reusableRecord.LostSamples != 0 {
			p.lostChunks += p.reusableRecord.LostSamples
			p.stats.ChunksLost += p.reusableRecord.LostSamples
			continue
		}
		p.stats.ChunksGot++

		chunk := tracerPktChunk{
			cpu: p.reusableRecord.CPU,
			buf: p.reusableRecord.RawSample,
		}

		var ok bool
		if ok, err = p.handlePktChunk(chunk); err != nil {
			log.Error().Err(err).Msg("handle chunk failed")
		} else if ok {
			p.stats.ChunksHandled++
		}
	}
}

// createPacketFromDecodedLayers creates a gopacket.Packet from the pre-parsed layers
// This is more efficient than gopacket.NewPacket as it reuses the already decoded layer data
func (p *PacketsPoller) createPacketFromDecodedLayers(data []byte, timestamp time.Time, cgroupID uint64, direction unixpacket.PacketDirection) gopacket.Packet {
	return bpf.CreatePacketFromDecodedLayers(data, timestamp, cgroupID, direction, p.decodedLayers, &p.ethLayer, &p.ipv4Layer, &p.ipv6Layer, &p.icmpv4Layer, &p.icmpv6Layer, &p.tcpLayer, &p.udpLayer, &p.sctpLayer, &p.dnsLayer, &p.radiusLayer, &p.payloadLayer)
}

func (p *PacketsPoller) checkBuffers() {
	// only bug in eBPF code can cause pktsMap overflow

	for {
		p.mtx.Lock()
		plen := len(p.pktsMap)
		p.mtx.Unlock()

		log.Debug().Int("size", plen).Msg("packets map size")
		if plen > 1024 {
			log.Error().Int("size", plen).Msg("packets map is too big, removig elements")
			p.mtx.Lock()
			for i, pkts := range p.pktsMap {
				pktBufferPool.Put(pkts)
				delete(p.pktsMap, i)
				if len(p.pktsMap) <= 1024 {
					break
				}
			}
			p.mtx.Unlock()
		}
		time.Sleep(5 * time.Second)
	}
}
